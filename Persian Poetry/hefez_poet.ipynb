{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d242fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from gensim.models import KeyedVectors\n",
    "import fasttext.util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16824fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276324"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.abspath(\"hafez.txt\")\n",
    "vocab_dir = \"hafez_vocab\"\n",
    "\n",
    "assert os.path.isfile(data_path), f\"{data_path} not found. Please upload the file.\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58dd4018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "data_path = \"hafez.txt\"\n",
    "vocab_dir = \"hafez_vocab\"\n",
    "\n",
    "tokenizer.train(files=[data_path], vocab_size=30000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "    \"<lb>\"\n",
    "])\n",
    "\n",
    "# Save the tokenizer model\n",
    "if not os.path.exists(vocab_dir):\n",
    "    os.makedirs(vocab_dir)\n",
    "tokenizer.save_model(vocab_dir)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(vocab_dir, \"vocab.json\"),\n",
    "    os.path.join(vocab_dir, \"merges.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacdfc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of padded verses: 4191\n",
      "Example padded verse: [177, 125, 129, 2010, 727, 5030, 483, 911, 3038, 6276, 289, 952, 3987, 263, 226, 33, 81, 71, 35, 296, 419, 1833, 1663, 1642, 831, 765, 1826, 307, 545, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "﻿الا يا ايها الساقی ادر کاسا و ناولها <lb> که عشق آسان نمود اول ولی افتاد مشکل‌ها<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# Split the text into verses using double newlines\n",
    "verses = text.split(\"\\n\\n\")\n",
    "\n",
    "# Replace single newlines within verses with <lb> and remove any empty verses\n",
    "verses = [verse.replace(\"\\n\", \" <lb> \") for verse in verses if verse.strip()]\n",
    "\n",
    "# Tokenize each verse\n",
    "tokenized_verses = [tokenizer.encode(verse).ids for verse in verses]\n",
    "\n",
    "# Pad or truncate the verses to have uniform length (max_len)\n",
    "max_len = 48\n",
    "padded_tokenized_verses = [verse + [tokenizer.token_to_id('<pad>')] * (max_len - len(verse)) for verse in tokenized_verses]\n",
    "\n",
    "print(f\"Number of padded verses: {len(padded_tokenized_verses)}\")\n",
    "print(f\"Example padded verse: {padded_tokenized_verses[0]}\")\n",
    "print(tokenizer.decode(padded_tokenized_verses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d94459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 2933\n",
      "Validation data size: 629\n",
      "Test data size: 629\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_data, test_val_data = train_test_split(padded_tokenized_verses, test_size=0.3, random_state=42)\n",
    "val_data, test_data       = train_test_split(test_val_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96d36209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_tensor = torch.tensor(train_data)\n",
    "val_tensor = torch.tensor(val_data)\n",
    "test_tensor = torch.tensor(test_data)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "val_dataset = TensorDataset(val_tensor)\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7faf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model_path = 'cc.fa.300.vec'  # Pre-trained FastText vectors Path\n",
    "fasttext_vectors = KeyedVectors.load_word2vec_format(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "414d18ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6630"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.get_vocab())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb60e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, fasttext_vectors, embedding_dim):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    word_to_idx = {word: idx for word, idx in vocab.items()}\n",
    "\n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word in fasttext_vectors:\n",
    "            embedding_matrix[idx] = fasttext_vectors[word]\n",
    "        else:\n",
    "            # Use a random vector or zero vector for unknown words\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# Example usage\n",
    "embedding_dim = 300  # Dimension of FastText embeddings\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, fasttext_vectors, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68dbbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pretrained_weights):\n",
    "        super(PretrainedEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_weights, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False  # Freeze the weights if you don't want them to be updated\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.encoding.size(1):\n",
    "            position = torch.arange(0, seq_len).unsqueeze(1).float().to(x.device)\n",
    "            div_term = torch.exp(torch.arange(0, self.encoding.size(2), 2).float() * -(math.log(10000.0) / self.encoding.size(2))).to(x.device)\n",
    "            encoding = torch.zeros(seq_len, self.encoding.size(2)).to(x.device)\n",
    "            encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "            encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "            encoding = encoding.unsqueeze(0)\n",
    "        else:\n",
    "            encoding = self.encoding[:, :seq_len, :]\n",
    "        return x + encoding.to(x.device)\n",
    "    \n",
    "class GPT2Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, pretrained_weights):\n",
    "        super(GPT2Embedding, self).__init__()\n",
    "        self.token_embedding = PretrainedEmbedding(vocab_size, d_model, pretrained_weights)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_encoding(token_embeddings)\n",
    "        embeddings = self.layer_norm(position_embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % nhead == 0\n",
    "        self.d_k = d_model // nhead\n",
    "        self.nhead = nhead\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.attention = None\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.linear_q(query).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        key = self.linear_k(key).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        value = self.linear_v(value).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        x, self.attention = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.nhead * self.d_k)\n",
    "        return self.linear_out(x), self.attention\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff=2048):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layer_norm1(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.layer_norm2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, d_ff, pretrained_weights):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = GPT2Embedding(vocab_size, d_model, max_len, pretrained_weights)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0e68ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = 30000\n",
    "d_model = 300\n",
    "nhead = 6\n",
    "num_layers = 6\n",
    "max_len = 48\n",
    "d_ff = 2048\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb6cd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb688640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Decoder(vocab_size, d_model, nhead, num_layers, max_len, d_ff, embedding_matrix)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc09dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.0012892781220295508\n",
      "Epoch 1/20, Validation Loss: 0.19201218858361244\n",
      "Epoch 2/20, Train Loss: 0.0013231848437420051\n",
      "Epoch 2/20, Validation Loss: 0.19722489304840565\n",
      "Epoch 3/20, Train Loss: 0.0015287892469053115\n",
      "Epoch 3/20, Validation Loss: 0.191148653998971\n",
      "Epoch 4/20, Train Loss: 0.0011166712502017617\n",
      "Epoch 4/20, Validation Loss: 0.18856663927435874\n",
      "Epoch 5/20, Train Loss: 0.0009097629669628551\n",
      "Epoch 5/20, Validation Loss: 0.1929286364465952\n",
      "Epoch 6/20, Train Loss: 0.0005074602203145016\n",
      "Epoch 6/20, Validation Loss: 0.1840894803404808\n",
      "Epoch 7/20, Train Loss: 0.0005788098601244754\n",
      "Epoch 7/20, Validation Loss: 0.18542455434799193\n",
      "Epoch 8/20, Train Loss: 0.0006118243945820723\n",
      "Epoch 8/20, Validation Loss: 0.1842499364167452\n",
      "Epoch 9/20, Train Loss: 0.0003031726324558764\n",
      "Epoch 9/20, Validation Loss: 0.1854371376335621\n",
      "Epoch 10/20, Train Loss: 0.0003317824791694242\n",
      "Epoch 10/20, Validation Loss: 0.18403465077280998\n",
      "Epoch 11/20, Train Loss: 0.0001969152051416408\n",
      "Epoch 11/20, Validation Loss: 0.18280148208141328\n",
      "Epoch 12/20, Train Loss: 0.0001847022050123601\n",
      "Epoch 12/20, Validation Loss: 0.18208560049533845\n",
      "Epoch 13/20, Train Loss: 0.0001794279073099302\n",
      "Epoch 13/20, Validation Loss: 0.18163841888308524\n",
      "Epoch 14/20, Train Loss: 0.0001785104183869882\n",
      "Epoch 14/20, Validation Loss: 0.18112559504806997\n",
      "Epoch 15/20, Train Loss: 0.00017178552377078196\n",
      "Epoch 15/20, Validation Loss: 0.18086330704391002\n",
      "Epoch 16/20, Train Loss: 0.0001692375123599524\n",
      "Epoch 16/20, Validation Loss: 0.18093054927885532\n",
      "Epoch 17/20, Train Loss: 0.00017571376915641494\n",
      "Epoch 17/20, Validation Loss: 0.18009096421301365\n",
      "Epoch 18/20, Train Loss: 0.00016082395972182934\n",
      "Epoch 18/20, Validation Loss: 0.18010890781879424\n",
      "Epoch 19/20, Train Loss: 0.00015837273222923218\n",
      "Epoch 19/20, Validation Loss: 0.18012945391237736\n",
      "Epoch 20/20, Train Loss: 0.00015259310052699774\n",
      "Epoch 20/20, Validation Loss: 0.1801888071000576\n",
      "Test Loss: 0.16987438946962358\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[0]\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_3.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_3.pt'))\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb5240d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): GPT2Embedding(\n",
       "    (token_embedding): PretrainedEmbedding(\n",
       "      (embedding): Embedding(30000, 300)\n",
       "    )\n",
       "    (position_encoding): PositionalEncoding()\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (linear_q): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (linear_k): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (linear_v): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (linear_out): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (linear1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_out): Linear(in_features=300, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_3.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "867622f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ای دل گر از آن چاه زنخدان به درآیی بود بروم پرست درآيی درآيی وای رمانی رمانی ارزد نوشت جاميدن بازرسان شست هوا برين چو دولت آمده مقامدم هوا چو بندگان بازرسان خداداده پاک هوا رقم حور گلرنگ بگشا غارت صوابسی سفله سيه الله هوا دم گرديدن بودی مستش رفتم معتقد عتاب ديده الف ازايتی جور سحر زرد\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, input_text, max_length=100, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(input_text).ids\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "\n",
    "    vocab_size = model.embedding.token_embedding.embedding.num_embeddings\n",
    "    generated = input_ids\n",
    "\n",
    "    for _ in range(max_length - len(input_ids[0])):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "\n",
    "            # Apply top-k sampling\n",
    "            top_k_probs, top_k_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), top_k)\n",
    "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "            next_token = top_k_indices.gather(1, next_token)\n",
    "\n",
    "            # Ensure the sampling does not generate out-of-vocabulary tokens\n",
    "            while next_token.item() >= vocab_size:\n",
    "                next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                next_token = top_k_indices.gather(1, next_token)\n",
    "\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.token_to_id('<eos>'):\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated.squeeze().tolist())\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"ای دل گر از آن چاه زنخدان به درآیی\"\n",
    "generated_text = generate_text(model, tokenizer, input_text, max_length=64, temperature=5, top_k=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d82f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
