{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e5e238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8017f6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276324"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.abspath(\"hafez.txt\")\n",
    "vocab_dir = \"hafez_vocab\"\n",
    "\n",
    "assert os.path.isfile(data_path), f\"{data_path} not found. Please upload the file.\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f1c2b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[data_path], vocab_size=30000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "    \"<lb>\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38405b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hafez_vocab/vocab.json', 'hafez_vocab/merges.txt']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tokenizer model\n",
    "if not os.path.exists(vocab_dir):\n",
    "    os.makedirs(vocab_dir)\n",
    "tokenizer.save_model(vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd12d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(vocab_dir, \"vocab.json\"),\n",
    "    os.path.join(vocab_dir, \"merges.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e5d8352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['ØŃØ§ÙģØ¸', 'ĠØ´Ø¨', 'ĠÙĩØ¬Ø±Ø§ÙĨ', 'ĠØ´Ø¯', 'ĠØ¨ÙĪÛĮ', 'ĠØ®ÙĪØ´', 'ĠÙĪØµÙĦ', 'ĠØ¢ÙħØ¯', 'Ġ', '<', 'l', 'b', '>', 'ĠØ´Ø§Ø¯', 'ÙĬØª', 'ĠÙħØ¨Ø§Ø±Ú©', 'ĠØ¨Ø§Ø¯', 'ĠØ§ÛĮ', 'ĠØ¹Ø§Ø´ÙĤ', 'ĠØ´ÙĬØ¯Ø§ÙĬÛĮ', 'Ġ', '<', 'l', 'b', '>', 'ĠØ§ÛĮ', 'ĠØ¯ÙĦ', 'ĠÚ¯Ø±', 'ĠØ§Ø²', 'ĠØ¢ÙĨ', 'ĠÚĨØ§Ùĩ', 'ĠØ²ÙĨØ®Ø¯Ø§ÙĨ', 'ĠØ¨Ùĩ', 'ĠØ¯Ø±Ø¢ÙĬÛĮ', 'Ġ', '<', 'l', 'b', '>', 'ĠÙĩØ±', 'ĠØ¬Ø§', 'ĠÚ©Ùĩ', 'ĠØ±ÙĪÛĮ', 'ĠØ²ÙĪØ¯', 'ĠÙ¾Ø´ÙĬÙħØ§ÙĨ', 'ĠØ¨Ùĩ', 'ĠØ¯Ø±Ø¢ÙĬÛĮ', 'Ġ', '<', 'l', 'b', '>', 'ĠÙĩØ´', 'ĠØ¯Ø§Ø±', 'ĠÚ©Ùĩ', 'ĠÚ¯Ø±', 'ĠÙĪØ³ÙĪØ³Ùĩ', 'ĠØ¹ÙĤÙĦ', 'ĠÚ©ÙĨÛĮ', 'ĠÚ¯ÙĪØ´', 'Ġ', '<', 'l', 'b', '>', 'ĠØ¢Ø¯Ùħ', 'ĠØµÙģØª', 'ĠØ§Ø²', 'ĠØ±ÙĪØ¶Ùĩ', 'ĠØ±Ø¶ÙĪØ§ÙĨ', 'ĠØ¨Ùĩ', 'ĠØ¯Ø±Ø¢ÙĬÛĮ']\n",
      "Token IDs: [487, 505, 1212, 403, 683, 420, 789, 508, 226, 33, 81, 71, 35, 1398, 539, 2742, 381, 413, 635, 4768, 226, 33, 81, 71, 35, 413, 338, 399, 311, 353, 1780, 2081, 304, 2074, 226, 33, 81, 71, 35, 418, 668, 296, 467, 2544, 3337, 304, 2074, 226, 33, 81, 71, 35, 2005, 390, 296, 399, 4866, 977, 1144, 547, 226, 33, 81, 71, 35, 1778, 2358, 311, 2401, 2957, 304, 2074]\n",
      "Decoded Text: حافظ شب هجران شد بوی خوش وصل آمد <lb> شاديت مبارک باد ای عاشق شيدايی <lb> ای دل گر از آن چاه زنخدان به درآيی <lb> هر جا که روی زود پشيمان به درآيی <lb> هش دار که گر وسوسه عقل کنی گوش <lb> آدم صفت از روضه رضوان به درآيی\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sample Persian text\n",
    "sample_text = \"\"\"حافظ شب هجران شد بوی خوش وصل آمد\n",
    "شاديت مبارک باد ای عاشق شيدايی\n",
    "ای دل گر از آن چاه زنخدان به درآيی\n",
    "هر جا که روی زود پشيمان به درآيی\n",
    "هش دار که گر وسوسه عقل کنی گوش\n",
    "آدم صفت از روضه رضوان به درآيی\"\"\"\n",
    "\n",
    "# Replace line breaks with <lb>\n",
    "sample_text = sample_text.replace(\"\\n\", \" <lb> \")\n",
    "\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"Token IDs: {encoded.ids}\")\n",
    "\n",
    "# Decode the token IDs back to text\n",
    "decoded_text = tokenizer.decode(encoded.ids)\n",
    "print(f\"Decoded Text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9635553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, tokenizer):\n",
    "    return [tokenizer.encode(sentence).ids for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ba8da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of padded verses: 4191\n",
      "Example padded verse: [177, 125, 129, 2010, 727, 5030, 483, 911, 3038, 6276, 289, 952, 3987, 263, 226, 33, 81, 71, 35, 296, 419, 1833, 1663, 1642, 831, 765, 1826, 307, 545, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "﻿الا يا ايها الساقی ادر کاسا و ناولها <lb> که عشق آسان نمود اول ولی افتاد مشکل‌ها<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# Split the text into verses using double newlines\n",
    "verses = text.split(\"\\n\\n\")\n",
    "\n",
    "# Replace single newlines within verses with <lb> and remove any empty verses\n",
    "verses = [verse.replace(\"\\n\", \" <lb> \") for verse in verses if verse.strip()]\n",
    "\n",
    "# Tokenize each verse\n",
    "tokenized_verses = [tokenizer.encode(verse).ids for verse in verses]\n",
    "\n",
    "# Pad or truncate the verses to have uniform length (max_len)\n",
    "max_len = 48\n",
    "padded_tokenized_verses = [verse + [tokenizer.token_to_id('<pad>')] * (max_len - len(verse)) for verse in tokenized_verses]\n",
    "\n",
    "print(f\"Number of padded verses: {len(padded_tokenized_verses)}\")\n",
    "print(f\"Example padded verse: {padded_tokenized_verses[0]}\")\n",
    "print(tokenizer.decode(padded_tokenized_verses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54f832c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 2933\n",
      "Validation data size: 629\n",
      "Test data size: 629\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_data, test_val_data = train_test_split(padded_tokenized_verses, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(test_val_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eaf85392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train dataloader: 367\n",
      "Number of batches in val dataloader: 79\n",
      "Number of batches in test dataloader: 79\n"
     ]
    }
   ],
   "source": [
    "def create_dataloader(data, batch_size):\n",
    "    input_ids = torch.tensor(data)\n",
    "    dataset = TensorDataset(input_ids)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = create_dataloader(train_data, batch_size)\n",
    "val_dataloader = create_dataloader(val_data, batch_size)\n",
    "test_dataloader = create_dataloader(test_data, batch_size)\n",
    "\n",
    "print(f\"Number of batches in train dataloader: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in val dataloader: {len(val_dataloader)}\")\n",
    "print(f\"Number of batches in test dataloader: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13ad59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # (max_len, d_model) => (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:, :seq_len, :].to(x.device)\n",
    "\n",
    "class GPT2Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super(GPT2Embedding, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_encoding(token_embeddings)\n",
    "        embeddings = self.layer_norm(position_embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# B : batch_size, H: nhead, D: d_model, S: seq_len, M: max_len\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % nhead == 0\n",
    "        self.d_k = d_model // nhead\n",
    "        self.nhead = nhead\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.attention = None\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k) # (B, H, S, d_k) * (B, H, d_k, S) => (B, H, S, S)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        # query, key, value: (B, S, D) -> (B, S, N, d_K) -> (B, H, S, d_K)\n",
    "        query = self.linear_q(query).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        key   = self.linear_k(key)  .view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        value = self.linear_v(value).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention = self.scaled_dot_product_attention(query, key, value, mask) # x: (B, H, S, d_k)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.nhead * self.d_k)#  x: (B, H, S, d_k) -> (B, S, H, d_k) -> (B, S, D)\n",
    "        return self.linear_out(x) # (B, S, D)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff=2048):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, d_ff=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = GPT2Embedding(vocab_size, d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "84d17782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, criterion, and optimizer\n",
    "model = Decoder(vocab_size, d_model, nhead, num_layers, max_len, d_ff)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0cec2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = 30000\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "max_len = 48\n",
    "d_ff = 2048\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# first model's results:\n",
    "# Train Loss: 0.002588495343548629\n",
    "# Val   Loss: 0.2399774716247486\n",
    "# Test  Loss: 0.22456781816067575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca993cf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[0]\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1c2f3297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): GPT2Embedding(\n",
       "    (token_embedding): Embedding(30000, 512)\n",
       "    (position_encoding): PositionalEncoding()\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_out): Linear(in_features=512, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, input_text, max_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(input_text).ids\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "\n",
    "    for _ in range(max_length - len(input_ids[0])):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.token_to_id('<eos>'):\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(input_ids.squeeze().tolist())\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"ای دل گر از آن چاه زنخدان به درآیی\"\n",
    "generated_text = generate_text(model, tokenizer, input_text, max_length=48, temperature=1.0)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b9e56551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, criterion, optimizer, and scheduler\n",
    "model = Decoder(vocab_size, d_model, nhead, num_layers, max_len, d_ff)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # Example scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "94dcddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model\n",
    "vocab_size = 20000\n",
    "d_model = 768\n",
    "nhead = 12 \n",
    "num_layers = 8\n",
    "max_len = 64\n",
    "d_ff = 3072\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0846d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.00018994479081469744\n",
      "Epoch 1/10, Validation Loss: 0.17879455501237249\n",
      "Epoch 2/10, Train Loss: 0.00018797446795588876\n",
      "Epoch 2/10, Validation Loss: 0.17801189200976228\n",
      "Epoch 3/10, Train Loss: 0.00018423027749511488\n",
      "Epoch 3/10, Validation Loss: 0.17763244370116463\n",
      "Epoch 4/10, Train Loss: 0.00018375191746114014\n",
      "Epoch 4/10, Validation Loss: 0.1780738322229325\n",
      "Epoch 5/10, Train Loss: 0.0001843025260492309\n",
      "Epoch 5/10, Validation Loss: 0.17754007121430168\n",
      "Epoch 6/10, Train Loss: 0.00018400431483998585\n",
      "Epoch 6/10, Validation Loss: 0.17763497998725764\n",
      "Epoch 7/10, Train Loss: 0.00018445390432994375\n",
      "Epoch 7/10, Validation Loss: 0.17818778762711754\n",
      "Epoch 8/10, Train Loss: 0.00018276167848389798\n",
      "Epoch 8/10, Validation Loss: 0.17832909632898583\n",
      "Epoch 9/10, Train Loss: 0.00018234126976715293\n",
      "Epoch 9/10, Validation Loss: 0.17818146142401273\n",
      "Epoch 10/10, Train Loss: 0.00018105168879375987\n",
      "Epoch 10/10, Validation Loss: 0.1782459442421228\n",
      "Early stopping triggered\n",
      "Test Loss: 0.17638141671313515\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping and learning rate scheduler\n",
    "num_epochs = 10\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[0]\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_2.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()  # Step the scheduler\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_2.pt'))\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), input_ids[:, 1:].reshape(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a608fe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): GPT2Embedding(\n",
       "    (token_embedding): Embedding(20000, 768)\n",
       "    (position_encoding): PositionalEncoding()\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_out): Linear(in_features=768, out_features=20000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_2.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "17e8f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ای دل گر از آن چاه زنخدان به درآيی وداع مگير بيفشانی خونين بسوزی لعورش قلمفراق کاری مغبازده مومعی صحيفه ببر توان شمار بريده جانادبير کلام\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"ای دل گر از آن چاه زنخدان به درآيی\"\n",
    "generated_text = generate_text(model, tokenizer, input_text, max_length=64, temperature=17)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d38403",
   "metadata": {},
   "source": [
    "ای دل گر از آن چاه زنخدان به درآيی\n",
    "وداع مگير بيفشانی خونين بسوزی\n",
    "لعورش قلمفراق کاری مغبازده مومعی صحيفه\n",
    "ببر توان شمار بريده جانا دبير کلام"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
