{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a7918fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from gensim.models import KeyedVectors\n",
    "import fasttext.util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd24fd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276324"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.abspath(\"hafez.txt\")\n",
    "vocab_dir = \"hafez_vocab\"\n",
    "\n",
    "assert os.path.isfile(data_path), f\"{data_path} not found. Please upload the file.\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4004de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the text\n",
    "data_path = \"hafez.txt\"\n",
    "vocab_dir = \"hafez_vocab\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Replace single newlines within verses with <lb> and add double <lb> after each pair of verses\n",
    "lines = text.split('\\n')\n",
    "structured_text = \"\"\n",
    "for i in range(0, len(lines), 2):\n",
    "    structured_text += lines[i].replace(\"\\n\", \" <lb> \") + \" <lb> \"\n",
    "    if i + 1 < len(lines):\n",
    "        structured_text += lines[i + 1].replace(\"\\n\", \" <lb> \") + \" <lb> <lb> \"\n",
    "\n",
    "# Save the structured text back to the file (or a new file)\n",
    "structured_data_path = \"structured_hafez.txt\"\n",
    "with open(structured_data_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(structured_text)\n",
    "\n",
    "# Train the tokenizer with structured data\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[structured_data_path], vocab_size=30000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "    \"<lb>\"\n",
    "])\n",
    "tokenizer.save_model(vocab_dir)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(vocab_dir, \"vocab.json\"),\n",
    "    os.path.join(vocab_dir, \"merges.txt\"),\n",
    ")\n",
    "\n",
    "# Load pre-trained FastText vectors using gensim\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('cc.fa.300.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "495fe549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "def create_embedding_matrix(tokenizer, fasttext_model, embedding_dim):\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    word_to_idx = tokenizer.get_vocab()\n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word in fasttext_model:\n",
    "            embedding_matrix[idx] = fasttext_model[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, fasttext_model, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c4a84d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding layer with pre-trained FastText embeddings\n",
    "class PretrainedEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pretrained_weights):\n",
    "        super(PretrainedEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_weights, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False  # Enable fine-tuning\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.encoding.size(1):\n",
    "            position = torch.arange(0, seq_len).unsqueeze(1).float().to(x.device)\n",
    "            div_term = torch.exp(torch.arange(0, self.encoding.size(2), 2).float() * -(math.log(10000.0) / self.encoding.size(2))).to(x.device)\n",
    "            encoding = torch.zeros(seq_len, self.encoding.size(2)).to(x.device)\n",
    "            encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "            encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "            encoding = encoding.unsqueeze(0)\n",
    "        else:\n",
    "            encoding = self.encoding[:, :seq_len, :]\n",
    "        return x + encoding.to(x.device)\n",
    "\n",
    "# Define GPT2Embedding with PretrainedEmbedding and PositionalEncoding\n",
    "class GPT2Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, pretrained_weights):\n",
    "        super(GPT2Embedding, self).__init__()\n",
    "        self.token_embedding = PretrainedEmbedding(vocab_size, d_model, pretrained_weights)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_encoding(token_embeddings)\n",
    "        embeddings = self.layer_norm(position_embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# Multi-head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % nhead == 0\n",
    "        self.d_k = d_model // nhead\n",
    "        self.nhead = nhead\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "        self.attention = None\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.linear_q(query).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        key = self.linear_k(key).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        value = self.linear_v(value).view(batch_size, -1, self.nhead, self.d_k).transpose(1, 2)\n",
    "        x, self.attention = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.nhead * self.d_k)\n",
    "        return self.linear_out(x), self.attention\n",
    "\n",
    "# Feed-forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff=2048):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.layer_norm1(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        x = self.layer_norm2(x)\n",
    "        return x\n",
    "\n",
    "# Define the Transformer Decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_len, d_ff, pretrained_weights):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = GPT2Embedding(vocab_size, d_model, max_len, pretrained_weights)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cfc33847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the text\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "verses = text.split(\"\\n\\n\")\n",
    "verses = [verse.replace(\"\\n\", \" <lb> \") for verse in verses if verse.strip()]\n",
    "tokenized_verses = [tokenizer.encode(verse).ids for verse in verses]\n",
    "max_len = 48\n",
    "padded_tokenized_verses = [verse + [tokenizer.token_to_id('<pad>')] * (max_len - len(verse)) for verse in tokenized_verses]\n",
    "\n",
    "data_tensor = torch.tensor(padded_tokenized_verses, dtype=torch.long)\n",
    "dataset = TensorDataset(data_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b476ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_common_suffix(word1, word2, min_suffix_length=1):\n",
    "    \"\"\"Check if two words have a common suffix of at least min_suffix_length characters.\"\"\"\n",
    "    if len(word1) < min_suffix_length or len(word2) < min_suffix_length:\n",
    "        return False\n",
    "    suffix1 = word1[-min_suffix_length:]\n",
    "    suffix2 = word2[-min_suffix_length:]\n",
    "    return suffix1[1] == suffix2[1]\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, base_loss, min_suffix_length=1):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.min_suffix_length = min_suffix_length\n",
    "\n",
    "    def get_last_non_pad_token(self, tokens, pad_token_id):\n",
    "        non_pad_tokens = [token for token in tokens if token != pad_token_id]\n",
    "        return non_pad_tokens[-1] if non_pad_tokens else pad_token_id\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        base_loss_value = self.base_loss(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "        # Additional rhyme loss\n",
    "        rhyme_loss_value = 0\n",
    "        batch_size = targets.size(0)\n",
    "        pad_token_id = tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        num_pairs = batch_size // 2\n",
    "        for i in range(0, num_pairs * 2, 2):\n",
    "            verse1_tokens = targets[i, :].tolist()\n",
    "            verse2_tokens = targets[i + 1, :].tolist()\n",
    "\n",
    "            verse1_end_token = self.get_last_non_pad_token(verse1_tokens, pad_token_id)\n",
    "            verse2_end_token = self.get_last_non_pad_token(verse2_tokens, pad_token_id)\n",
    "\n",
    "            word1 = tokenizer.decode([verse1_end_token])\n",
    "            word2 = tokenizer.decode([verse2_end_token])\n",
    "\n",
    "            if not has_common_suffix(word1, word2, self.min_suffix_length):\n",
    "                rhyme_loss_value += 1\n",
    "\n",
    "        rhyme_loss_value = torch.tensor(rhyme_loss_value, dtype=torch.float, requires_grad=True).to(outputs.device)\n",
    "        total_loss = base_loss_value + rhyme_loss_value\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "971b3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "d_model = embedding_dim  # Match embedding_dim to FastText embedding size\n",
    "nhead = 12\n",
    "num_layers = 8\n",
    "max_len = 48\n",
    "d_ff = 256\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c39cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size, d_model, nhead, num_layers, max_len, d_ff, embedding_matrix)\n",
    "base_criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('<pad>'))\n",
    "criterion = CustomLoss(base_criterion, min_suffix_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2c0207b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8916ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 3.971500261266647\n",
      "Epoch 1/50, Validation Loss: 2.6457087891442437\n",
      "Epoch 2/50, Train Loss: 2.0449072394635626\n",
      "Epoch 2/50, Validation Loss: 1.8419234952756336\n",
      "Epoch 3/50, Train Loss: 1.426583420102949\n",
      "Epoch 3/50, Validation Loss: 1.4829259634372733\n",
      "Epoch 4/50, Train Loss: 1.1206104816871123\n",
      "Epoch 4/50, Validation Loss: 1.2843833408894993\n",
      "Epoch 5/50, Train Loss: 0.9741841191387022\n",
      "Epoch 5/50, Validation Loss: 1.1677143369978737\n",
      "Epoch 6/50, Train Loss: 0.8936705664003063\n",
      "Epoch 6/50, Validation Loss: 1.0917890522629023\n",
      "Epoch 7/50, Train Loss: 0.8581034447135807\n",
      "Epoch 7/50, Validation Loss: 1.0577826347950448\n",
      "Epoch 8/50, Train Loss: 0.8404673229749337\n",
      "Epoch 8/50, Validation Loss: 1.0498670891707576\n",
      "Epoch 9/50, Train Loss: 0.8388667127958261\n",
      "Epoch 9/50, Validation Loss: 1.048213306439062\n",
      "Epoch 10/50, Train Loss: 0.8402662197565182\n",
      "Epoch 10/50, Validation Loss: 1.0458772321670646\n",
      "Epoch 11/50, Train Loss: 0.8583445011715449\n",
      "Epoch 11/50, Validation Loss: 1.0527570751754043\n",
      "Epoch 12/50, Train Loss: 0.8599436737382177\n",
      "Epoch 12/50, Validation Loss: 1.0558599477846136\n",
      "Epoch 13/50, Train Loss: 0.8520997007567948\n",
      "Epoch 13/50, Validation Loss: 1.0545046501369921\n",
      "Epoch 14/50, Train Loss: 0.8449004834332241\n",
      "Epoch 14/50, Validation Loss: 1.0571704767656802\n",
      "Epoch 15/50, Train Loss: 0.8418995361371002\n",
      "Epoch 15/50, Validation Loss: 1.0614070985524091\n",
      "Early stopping triggered\n",
      "Test Loss: 1.0483192050308086\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, input_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[0]\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, input_ids)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model_6.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_6.pt'))\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0]\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, input_ids)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "12460db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): GPT2Embedding(\n",
       "    (token_embedding): PretrainedEmbedding(\n",
       "      (embedding): Embedding(6056, 300)\n",
       "    )\n",
       "    (position_encoding): PositionalEncoding()\n",
       "    (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (linear_q): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (linear_k): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (linear_v): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (linear_out): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (linear1): Linear(in_features=300, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=256, out_features=300, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_out): Linear(in_features=300, out_features=6056, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_5.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "b1471ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "رقص بر شعر تر و ناله نی خوش باشد مژده گم کايامبسته دريغا\n",
      " لبم کاری کشيد ملول يعنی بيف معدن نگهبان پيوست پيوست راهروانسامان فقارزد\n",
      "\n",
      "لمنقش مهربان مرنج بجز گيسويتروند جويبار خصم شکسته عاشقم برانداز گفتی\n",
      " غريق بخش شادی روا ناف بگرداند گردی بخشش سران بلاغتدهیاککشان بگوييم\n",
      "\n",
      " نخو عافيتت بگشای عشقت عود عود اميدوار فکنم\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to generate text with desired structure and repetition penalty\n",
    "def generate_text(model, tokenizer, input_text, max_length=100, temperature=1.0, top_k=50, repetition_penalty=1.2):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(input_text).ids\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "\n",
    "    vocab_size = model.embedding.token_embedding.embedding.num_embeddings\n",
    "    generated = input_ids\n",
    "\n",
    "    line_length = 15  # Adjust according to desired line length\n",
    "    verse_count = 0\n",
    "    token_usage = {}  # Track token usage\n",
    "\n",
    "    for _ in range(max_length - len(input_ids[0])):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated)\n",
    "            next_token_logits = outputs[:, -1, :] / temperature\n",
    "\n",
    "            # Apply repetition penalty\n",
    "            for token_id in generated[0].tolist():\n",
    "                next_token_logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "            # Apply top-k sampling\n",
    "            top_k_probs, top_k_indices = torch.topk(F.softmax(next_token_logits, dim=-1), top_k)\n",
    "            next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "            next_token = top_k_indices.gather(1, next_token)\n",
    "\n",
    "            # Ensure the sampling does not generate out-of-vocabulary tokens\n",
    "            while next_token.item() >= vocab_size:\n",
    "                next_token = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                next_token = top_k_indices.gather(1, next_token)\n",
    "\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            # Update token usage\n",
    "            token_id = next_token.item()\n",
    "            token_usage[token_id] = token_usage.get(token_id, 0) + 1\n",
    "\n",
    "            if next_token.item() == tokenizer.token_to_id('<eos>'):\n",
    "                break\n",
    "\n",
    "            # Insert line breaks at appropriate intervals\n",
    "            if generated.size(1) % line_length == 0:\n",
    "                generated = torch.cat((generated, torch.tensor([[tokenizer.token_to_id('<lb>')]], device=generated.device)), dim=1)\n",
    "                verse_count += 1\n",
    "\n",
    "            # Insert double line break after every two verses\n",
    "            if verse_count % 2 == 0 and verse_count > 0:\n",
    "                generated = torch.cat((generated, torch.tensor([[tokenizer.token_to_id('<lb>')]], device=generated.device)), dim=1)\n",
    "                verse_count = 0\n",
    "\n",
    "    generated_text = tokenizer.decode(generated.squeeze().tolist())\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"رقص بر شعر تر و ناله نی خوش باشد\"\n",
    "generated_text = generate_text(model, tokenizer, input_text, max_length=64, temperature=12.0, top_k=100, repetition_penalty=1)\n",
    "print(generated_text.replace(\"<lb>\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88259304",
   "metadata": {},
   "source": [
    "ای دل گر از آن چاه زنخدان به درآییی گفتند ذکر ورد اندر\n",
    "رقص ترک فنا مرد آسايش مکتب فلانی پاک سمنمر پر موی مر توانگر\n",
    "\n",
    "رقص بر شعر تر و ناله نی خوش باشد نرگس خاک نکنی شرابخانه\n",
    "نشنيد چنان مشرف کمند دريغ فوت هيچش مصطبه دوساله مسال ميسرم\n",
    "\n",
    "شد لشکر غم بی عدد از بخت می‌خواهم مدد خار جگر هواست\n",
    "زنجير زدند نسيم صنمی صبوری نوميد دشواريری خطرناک کفر آنچ شير پديد قلندر"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d12b9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
